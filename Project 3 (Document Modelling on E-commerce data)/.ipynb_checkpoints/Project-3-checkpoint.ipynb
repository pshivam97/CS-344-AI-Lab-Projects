{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "80ed4c8ac3efecfc5eda32b79671fd4d01e6fa6c"
   },
   "source": [
    "# CS-386 Artificial Intelligence Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d2f6064d96e278f35a5dc4e3aeb6ef87e33c8a8"
   },
   "source": [
    "# Project - 3 : Mining E-Commerce Customer Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ac30fb31efcf07f9234ed9e1ef1b79aac339a4a"
   },
   "source": [
    "# Shivam Pandey (160010003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49b529403b021c793ac0a9cdec076a0ce5b8b8cc"
   },
   "source": [
    "### Question 1: Preprocess the corpus of customer reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../input/Womens Clothing E-Commerce Reviews.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3668674331b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mClothing_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m862\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mcorpus_pandas_dataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/Womens Clothing E-Commerce Reviews.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mcorpus_with_particular_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_pandas_dataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus_pandas_dataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Clothing ID\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mClothing_ID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#corpus_with_particular_ID = corpus_pandas_dataFrame.dropna(subset=['Review Text'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../input/Womens Clothing E-Commerce Reviews.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "            ## Importing all necessary python libraries\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import time\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "            ## Pre-processing entails \"Removing Punctuations\", \"Lower-Casing\"\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "Clothing_ID = 862\n",
    "\n",
    "corpus_pandas_dataFrame = pd.read_csv(\"../input/Womens Clothing E-Commerce Reviews.csv\")\n",
    "corpus_with_particular_ID = corpus_pandas_dataFrame[corpus_pandas_dataFrame[\"Clothing ID\"] == Clothing_ID]\n",
    "#corpus_with_particular_ID = corpus_pandas_dataFrame.dropna(subset=['Review Text'])\n",
    "corpus_with_particular_ID = corpus_with_particular_ID.dropna(subset=['Review Text'])\n",
    "review_text = corpus_with_particular_ID[\"Review Text\"]\n",
    "\n",
    "corpus = list() # This will be a list of strings, where each string will be a \"document\"\n",
    "\n",
    "## Pre-processing Corpus\n",
    "for each_review in review_text:\n",
    "    preprocessed_review = each_review.lower() # Lower-casing each document\n",
    "    preprocessed_review = re.sub(r'[^A-Za-z ]', '', preprocessed_review) # removing Punctuations from each document\n",
    "    corpus.append(preprocessed_review)\n",
    "\n",
    "print(corpus) # print the pre-processed corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd0113518f36f3343f1ad5be1739948d092c66e4"
   },
   "source": [
    "### Question 2: Remove stopwords, standardize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "                ## Stemming and Lemmatization - Applying to the pre-processed corpus\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for each_document_index in range(len(corpus)):\n",
    "    list_of_words = corpus[each_document_index].strip().split()\n",
    "    \n",
    "    for each_word_index in range(len(list_of_words)) :\n",
    "        list_of_words[each_word_index] = lemmatizer.lemmatize(list_of_words[each_word_index])\n",
    "    \n",
    "    corpus[each_document_index] = ' '.join(list_of_words)\n",
    "\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "                                # STOPWORDS Removal from the Corpus #\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "## Pre-processing Stopwords\n",
    "for stopWord_index in range(len(stopWords)):\n",
    "    stopWords[stopWord_index] = stopWords[stopWord_index].lower() # Lower-casing\n",
    "    stopWords[stopWord_index] = re.sub(r'[^A-Za-z ]','',stopWords[stopWord_index]) # removing Punctuations\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0, stop_words=stopWords, strip_accents='ascii')\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "                        ## Building the Vocabulary out of the corpus \n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "docs_tf = vectorizer.fit_transform(corpus)\n",
    "vocabulary_terms = vectorizer.get_feature_names()\n",
    "print(vocabulary_terms) # Its a list of \"vocabulary words\" developed from the corpus\n",
    "\n",
    "key_words = ['lovely','top']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "99d34954fafa38bf7f44911a54e7142e9c09bb3f"
   },
   "source": [
    "### Question 3: Build the Term-Frequency Inverse-Document-Frequency (TF-IDF) matrix and apply the Latent Semantic Analysis (LSA) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "808719509872026ac48c1a94bbb251fefeebbcd5"
   },
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "        ## Building the Term-Frequency Inverse-Document-Frequency (TF-IDF) matrix and performaing IR\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "# Composite model for both the corpus and the query \n",
    "docs_query_tf = vectorizer.transform(corpus + [' '.join(key_words)]) \n",
    "transformer = TfidfTransformer(smooth_idf = False)\n",
    "tfidf = transformer.fit_transform(docs_query_tf.toarray())\n",
    "\n",
    "# D(no. of documents) x V(cardinality of vocabulary set) document-term matrix\n",
    "tfidf_matrix = tfidf.toarray()[:-1] # Excluding the last column which contains the \n",
    "print(len(tfidf_matrix))\n",
    "# 1 x V query-term vector \n",
    "query_tfidf = tfidf.toarray()[-1] \n",
    "\n",
    "#print (tfidf_matrix)\n",
    "#print (query_tfidf)\n",
    "\n",
    "TFIDF_start_time = time.time()\n",
    "query_doc_tfidf_cos_dist = [cosine(query_tfidf, doc_tfidf) for doc_tfidf in tfidf_matrix]\n",
    "query_doc_tfidf_sort_index = np.argsort(np.array(query_doc_tfidf_cos_dist))\n",
    "TFIDF_end_time = time.time()\n",
    "\n",
    "#print(query_doc_tfidf_sort_index)\n",
    "\n",
    "print(\"#-------------------------------------------------------------------------------------------------------\\n\\t## Building the Term-Frequency Inverse-Document-Frequency (TF-IDF) matrix and performaing IR\\n#-------------------------------------------------------------------------------------------------------\")\n",
    "for rank, sort_index in enumerate(query_doc_tfidf_sort_index):\n",
    "    print (rank, query_doc_tfidf_cos_dist[sort_index], corpus[sort_index])\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "                ## Performing IR using LSA with SVD on TF-matrix\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "tf_matrix = docs_tf.toarray() # D x V matrix \n",
    "A = tf_matrix.T \n",
    "\n",
    "U, s, V = np.linalg.svd(A, full_matrices=1, compute_uv=1)\n",
    "\n",
    "K = 2 # number of components\n",
    "\n",
    "A_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), V[:K, :])) # V x D matrix (Reconstruction Matrix)\n",
    "docs_rep = np.dot(np.diag(s[:K]), V[:K, :]).T # D x K matrix \n",
    "terms_rep = np.dot(U[:,:K], np.diag(s[:K])) # V x K matrix \n",
    "\n",
    "key_word_indices = [vocabulary_terms.index(key_word) for key_word in key_words] # vocabulary indices \n",
    "key_words_rep = terms_rep[key_word_indices,:]\n",
    "query_rep = np.sum(key_words_rep, axis = 0)\n",
    "#print (query_rep)\n",
    "\n",
    "TF_LSA_start_time = time.time()\n",
    "query_doc_cos_dist = [cosine(query_rep, doc_rep) for doc_rep in docs_rep]\n",
    "query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "TF_LSA_end_time = time.time()\n",
    "\n",
    "print(\"#-----------------------------------------------------------------------------------------------------\\n\\t## Performing IR using LSA with SVD on TF-matrix\\n#-----------------------------------------------------------------------------------------------------\")\n",
    "for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "    print (rank, query_doc_cos_dist[sort_index], corpus[sort_index])\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot()\n",
    "plt.scatter(docs_rep[:,0], docs_rep[:,1], c=query_doc_cos_dist) # all documents \n",
    "plt.scatter(query_rep[0], query_rep[1], marker='+', c='red') # the query \n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "\n",
    "print(\"TFIDF Running Time :\",TFIDF_end_time-TFIDF_start_time,\"\\nLSA on TF-matrix running time :\",TF_LSA_end_time-TF_LSA_start_time,\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
